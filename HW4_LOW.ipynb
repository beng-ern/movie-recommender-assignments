{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from movie.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cols = ['movie_id', 'ranking', 'movie_title', 'country', 'release_year', 'film_rate', 'runtime', 'synopsis']\n",
    "\n",
    "df1 = pd.read_csv('movie.csv',  sep='|', names=r_cols, encoding='utf-8-sig', skiprows=1)\n",
    "\n",
    "# creating a new copy so that any changes to original data will not affect the train and test set below\n",
    "full_data1 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keywords.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cols = ['keyword_id', 'keywords', 'movie_id']\n",
    "\n",
    "df2 = pd.read_csv('keywords.csv',  sep='|', names=s_cols, encoding='utf-8-sig', skiprows=1)\n",
    "\n",
    "# concatenate and group the keywords for each movie ID\n",
    "full_data2 = df2.groupby('movie_id').agg({'keywords': lambda x: ' '.join(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies without keywords\n",
    "df1[~df1['movie_id'].isin(df2['movie_id'])][['movie_id', 'movie_title', 'ranking']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from plotsummary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cols = ['plot_id', 'movie_id', 'plot_text']\n",
    "\n",
    "df3 = pd.read_csv('plotsummary.csv',  sep='|', names=p_cols, encoding='utf-8-sig', skiprows=1)\n",
    "\n",
    "# concatenate and group the plot for each movie ID\n",
    "full_data3 = df3.groupby('movie_id').agg({'plot_text': lambda x: ' '.join(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list out the Top 10 movies by ranking\n",
    "top10_movies = full_data1[['ranking', 'movie_id', 'movie_title']].sort_values(by='ranking').head(10)\n",
    "top10_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: NLP Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "removal = str.maketrans('', '', string.punctuation)\n",
    "full_data2['keywords'] = full_data2['keywords'].str.translate(removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Calculate TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords with length less than 2 will be filtered out here\n",
    "tf_q1 = CountVectorizer()\n",
    "matrix_q1 = tf_q1.fit_transform(full_data2['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Calculate cosine similarity based on TF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_q1 = cosine_similarity(matrix_q1, matrix_q1)\n",
    "cosine_sim_q1_df = pd.DataFrame(cosine_sim_q1, columns=full_data2.movie_id, index=full_data2.movie_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: 10 Closest Movies Recommendation for Top 10 Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame\n",
    "results_q1 = pd.DataFrame(columns=['movie_name', 'movie_ID', '10_closest_movies'])\n",
    "\n",
    "# recommendation loop\n",
    "for j, k in zip(top10_movies['movie_title'], top10_movies['movie_id']):\n",
    "    \n",
    "    # sort the cosine similarity in descending order\n",
    "    recommend_df_q1 = pd.DataFrame(cosine_sim_q1_df[k].sort_values(ascending=False)) \n",
    "    \n",
    "    # reset index and rename the columns\n",
    "    recommend_df_q1.reset_index(level=0, inplace=True)\n",
    "    recommend_df_q1.columns = ['movie_id','distance_score']\n",
    "\n",
    "    # take the 10 nearest movies, excluding the movie itself\n",
    "    recommend_df_q1 = recommend_df_q1[recommend_df_q1['movie_id'] != k].head(10)\n",
    "    \n",
    "    # insert a new column for storing movie_title, map it from 'movies.csv'\n",
    "    recommend_df_q1.insert(loc=0, column='movie_title', value='')\n",
    "    recommend_df_q1['movie_title'] = recommend_df_q1['movie_id'].map(full_data1.set_index('movie_id')['movie_title'])\n",
    "    \n",
    "    # create a list of tuples in the order of movie_title, movie_id and distance_score\n",
    "    recommend_list_q1 = list(recommend_df_q1.itertuples(index=False, name=None))\n",
    "    \n",
    "    # append row to the dataframe\n",
    "    new_row_q1 = {'movie_name':j, 'movie_ID':k, '10_closest_movies':recommend_list_q1}\n",
    "    results_q1 = results_q1.append(new_row_q1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "results_q1\n",
    "\n",
    "# output to csv\n",
    "# results_q1.to_csv (r'C:\\Users\\Owner\\Desktop\\question1.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging both synopsis and plot into one column for cleaning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_plot = pd.merge(full_data1, full_data3, on=\"movie_id\", how=\"left\")\n",
    "\n",
    "# fill in the NaN values with white space especially those without plot or synopsis, or both\n",
    "synopsis_plot.fillna(' ', inplace=True)\n",
    "\n",
    "# create a new column for storing combined terms from synopsis and plot\n",
    "synopsis_plot['combined'] = ''\n",
    "synopsis_plot['combined'] = synopsis_plot['synopsis'] + ' ' + synopsis_plot['plot_text']\n",
    "\n",
    "# remove unnecessary columns for calculation afterwards\n",
    "synopsis_plot = synopsis_plot.drop(columns=['synopsis', 'plot_text', 'runtime', 'film_rate', 'release_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies without plot AND synopsis\n",
    "print(\"Number of movies without plot AND synopsis: \", end=\"\")\n",
    "print(synopsis_plot['movie_id'][synopsis_plot['combined'].str.strip(' ') == ''].count())\n",
    "print('\\n\\n', 'List of movies without plot AND synopsis:')\n",
    "print(synopsis_plot[['movie_id', 'movie_title', 'ranking']][synopsis_plot['combined'].str.strip(' ') == ''])\n",
    "\n",
    "# remove movies without plot and synopsis\n",
    "synopsis_plot.drop(synopsis_plot.loc[synopsis_plot['combined'].str.strip(' ') == ''].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: NLP Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Tokenization and Cleaning using Combined Terms from Synopsis and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create a list with lists of filtered keywords of each movie_id\n",
    "filtered_keywords_q2 = []\n",
    "\n",
    "# recommendation loop\n",
    "for i in synopsis_plot['combined']:\n",
    "    # replace \" !--Line Break--! \" with whitespace\n",
    "    raw_text = re.sub(\"!--Line Break--!\", \" \", i)\n",
    "\n",
    "    # split by whitespace\n",
    "    tokens = raw_text.split()\n",
    "\n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    # normalizing case\n",
    "    words = [w.lower() for w in stripped]\n",
    "\n",
    "    # filter out stop words ==FIRST TIME==\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "    # filter out stop words ==SECOND TIME==\n",
    "    words = [w for w in lemmatized if not w in stop_words]\n",
    "\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(w) for w in words]\n",
    "    \n",
    "    filtered_keywords_q2.append(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new dataframe with only movie ID and filtered keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_plot_update = pd.DataFrame(synopsis_plot[['movie_id']], \n",
    "                                 columns=['movie_id', 'keywords'])\n",
    "synopsis_plot_update['keywords'] = filtered_keywords_q2\n",
    "\n",
    "# array of keywords to be used in TfidfVectorizer later\n",
    "synopsis_plot_update['keywords'] = synopsis_plot_update['keywords'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other stop words match the ones from TfidfVectorizer will be filtered out again here\n",
    "# keywords with length less than 2 will be filtered out too\n",
    "tf_q2 = TfidfVectorizer(stop_words='english')\n",
    "matrix_q2 = tf_q2.fit_transform(synopsis_plot_update['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in stop words from TfidfVectorizer\n",
    "sets=[tf_q2.get_stop_words()]\n",
    "print([list(x) for x in sets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Calculate cosine similarity based on TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_q2 = linear_kernel(matrix_q2, matrix_q2)\n",
    "cosine_sim_q2_df = pd.DataFrame(cosine_sim_q2, columns=synopsis_plot_update.movie_id, index=synopsis_plot_update.movie_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: 10 Closest Movies Recommendation for Top 10 Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_q2 = pd.DataFrame(columns=['movie_name', 'movie_ID', '10_closest_movies'])\n",
    "\n",
    "# recommendation loop\n",
    "for j, k in zip(top10_movies['movie_title'], top10_movies['movie_id']):\n",
    "    \n",
    "    # sort the cosine similarity in descending order\n",
    "    recommend_df_q2 = pd.DataFrame(cosine_sim_q2_df[k].sort_values(ascending=False)) \n",
    "    \n",
    "    # reset index and rename columns\n",
    "    recommend_df_q2.reset_index(level=0, inplace=True)\n",
    "    recommend_df_q2.columns = ['movie_id','distance_score']\n",
    "\n",
    "    # take the 10 nearest movies, excluding the movie itself\n",
    "    recommend_df_q2 = recommend_df_q2[recommend_df_q2['movie_id'] != k].head(10)\n",
    "    \n",
    "    # insert a new column for storing movie_title, map it from 'movies.csv'\n",
    "    recommend_df_q2.insert(loc=0, column='movie_title', value='')\n",
    "    recommend_df_q2['movie_title'] = recommend_df_q2['movie_id'].map(full_data1.set_index('movie_id')['movie_title'])\n",
    "\n",
    "    # create a list of tuples in the order of movie_title, movie_id and distance_score\n",
    "    recommend_list_q2 = list(recommend_df_q2.itertuples(index=False, name=None))\n",
    "    \n",
    "    #append row to the dataframe\n",
    "    new_row_q2 = {'movie_name':j, 'movie_ID':k, '10_closest_movies':recommend_list_q2}\n",
    "    results_q2 = results_q2.append(new_row_q2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "results_q2\n",
    "\n",
    "# output as csv file\n",
    "# results_q2.to_csv (r'C:\\Users\\Owner\\Desktop\\question2.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
