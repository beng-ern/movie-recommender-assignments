{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import correlation, cosine\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cols = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df0 = pd.read_csv('u.data',  sep='\\t', names=r_cols, encoding='latin-1')\n",
    "\n",
    "# creating a new copy so that any changes to original data 'u.item' will not affect the train and test set below\n",
    "full_data = df0.copy()\n",
    "\n",
    "full_data[['user_id', 'item_id', 'rating']] = full_data[['user_id', 'item_id', 'rating']].astype(dtype = 'int64')\n",
    "\n",
    "# sort by timestamp for splitting into training and testing data set later\n",
    "full_data = full_data.sort_values(by = 'timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset in the way that users' latest ratings are in the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(list(full_data.user_id)):\n",
    "    sub_df = full_data[full_data.user_id == i]\n",
    "    len_df = sub_df.shape[0]\n",
    "    if i == 1:        \n",
    "        movieTrain = sub_df.head(round(len_df*0.7))\n",
    "        movieTest = sub_df.tail(len_df - round(len_df*0.7))\n",
    "    else:\n",
    "        movieTrain = pd.concat([movieTrain, sub_df.head(round(len_df*0.7))])\n",
    "        movieTest = pd.concat([movieTest, sub_df.tail(len_df - round(len_df*0.7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the user-item matrix based on training dataset\n",
    "\n",
    "# item-based\n",
    "UIM_train = movieTrain.pivot_table(index='user_id', columns='item_id', values='rating')\n",
    "\n",
    "# user_based\n",
    "IUM_train = UIM_train.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1", "Build a simple version of the collaborative filter in such a way that outputs the mean rating for the movie by all the users who have rated it. Note that here the ratings of each user is assigned an equal weight. If some movies are available only in the test set and not in the training set, assign the default rating of 3.0. What is the RMSE score obtained by this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using mean as predictor (before filter):1.068078\n",
      "RMSE using mean as predictor (after filter):1.064576\n"
     ]
    }
   ],
   "source": [
    "# minimum number of ratings of each movie\n",
    "t = 20\n",
    "\n",
    "# getting the mean rating of each movie in training set\n",
    "train_mean = movieTrain.groupby('item_id', sort=True).agg(rating_mean=('rating', 'mean'), \n",
    "                                                          frequency=('rating', 'count')).reset_index()\n",
    "train_mean_filtered = train_mean[train_mean['frequency'] > t]\n",
    "\n",
    "\n",
    "# setting the default rating\n",
    "def_rating = 3.0\n",
    "\n",
    "# left join testing set with training set mean rating\n",
    "merge_test = pd.merge(movieTest, train_mean, on=\"item_id\", how='left')\n",
    "\n",
    "# replace the NaN values in mean rating column due to left join (certain movies in testing set are NOT in training set)\n",
    "merge_test = merge_test.fillna(value={'rating_mean': def_rating})\n",
    "\n",
    "# left join testing set with training set mean rating\n",
    "merge_test0 = pd.merge(movieTest, train_mean_filtered, on=\"item_id\", how='left')\n",
    "\n",
    "# replace the NaN values in mean rating column due to left join (certain movies in testing set are NOT in training set)\n",
    "merge_test0 = merge_test0.fillna(value={'rating_mean': def_rating})\n",
    "\n",
    "y_true1 = merge_test['rating']\n",
    "y_hat1 = merge_test['rating_mean']\n",
    "\n",
    "y_true01 = merge_test0['rating']\n",
    "y_hat01 = merge_test0['rating_mean']\n",
    "\n",
    "rmse1 = sqrt(mean_squared_error(y_true1, y_hat1))\n",
    "print(\"RMSE using mean as predictor (before filter):%f\" %rmse1)\n",
    "\n",
    "rmse01 = sqrt(mean_squared_error(y_true01, y_hat01))\n",
    "print(\"RMSE using mean as predictor (after filter):%f\" %rmse01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions for prediction, finding k nearest neighbors, RSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(userid, itemid, IUM, mean, PCC, k=10):\n",
    "    if itemid in IUM.index:\n",
    "        # users in training set who rated movie 'itemid' and the item's ratings\n",
    "        rated_users = pd.DataFrame(IUM.loc[itemid].dropna())\n",
    "        rated_users.columns = ['rating']\n",
    "        \n",
    "        # min possible k nearest neighbors, adjustment if list of rated users less than input k\n",
    "        k = min(k, len(rated_users))\n",
    "        \n",
    "        # pass only the similarities of the users who rated target item into next function\n",
    "        pearson_mat_all = PCC[userid].loc[rated_users.index]\n",
    "        # filter out negative similarities\n",
    "        pearson_mat_all = pearson_mat_all[pearson_mat_all > 0]\n",
    "        \n",
    "        # getting sum of similarities and list of k nearest neighbors\n",
    "        list_of_sim, sum_sim, knn_users_list = findksimilarusers(userid, itemid, IUM, pearson_mat_all, k)\n",
    "        \n",
    "        # target user mean rating\n",
    "        target_user_mean_rating = mean.loc[userid]\n",
    "        \n",
    "        # nearest neigbors mean ratings\n",
    "        knn_user_mean_rating = mean.loc[knn_users_list]\n",
    "        \n",
    "        # weighted ratings of the selected item for nearest neigbors\n",
    "        centered_ratings = (rated_users.loc[knn_users_list]['rating'] - knn_user_mean_rating['rating_mean']).values\n",
    "        \n",
    "        # numerator\n",
    "        numer = np.dot(list_of_sim.values, centered_ratings)\n",
    "        \n",
    "        y_hat2 = target_user_mean_rating + (numer / sum_sim)\n",
    "        \n",
    "        return y_hat2[0]\n",
    "              \n",
    "    else:\n",
    "        return 3.0\n",
    "    \n",
    "    \n",
    "def findksimilarusers(userid, itemid, IUM, corr, k=10): \n",
    "    # sort descending without 'userid' input\n",
    "    pearson_mat = corr.sort_values(ascending=False)\n",
    "    \n",
    "    # nearest k neighbors\n",
    "    pearson_mat = pearson_mat.head(k)\n",
    "    \n",
    "    # sum of the similarities\n",
    "    sum_sim = pearson_mat.sum()\n",
    "    \n",
    "    # userid of the k nearest neighbors\n",
    "    knn_users = pearson_mat.index.values\n",
    "    \n",
    "    return pearson_mat, sum_sim, knn_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_filtered(y_hat_arr, y_true_arr):\n",
    "    gate = {'predicted_rating': y_hat_arr, 'actual_rating': y_true_arr}\n",
    "    df_yy = pd.DataFrame(data=gate)\n",
    "    df_yy = df_yy.dropna()\n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(df_yy['actual_rating'], df_yy['predicted_rating']))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training dataset for checking\n",
    "trainingset = movieTrain['item_id']\n",
    "\n",
    "# mean ratings of user\n",
    "train_user_mean = movieTrain.groupby('user_id', sort=True).agg(rating_mean=('rating', 'mean'))\n",
    "\n",
    "# compute the Pearson Correlation Coefficient\n",
    "pearson_mat_all = IUM_train.corr(method='pearson')\n",
    "\n",
    "UI_pairs = zip(movieTest['user_id'].values, movieTest['item_id'].values)\n",
    "y_hat2 = []\n",
    "\n",
    "for j,k in UI_pairs:\n",
    "    y_hat2.append(prediction(j, k, IUM_train, train_user_mean, pearson_mat_all))\n",
    "\n",
    "y_hat2 = np.array(y_hat2)\n",
    "y_true2 = np.array(movieTest['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the RSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for PCC user-based collaborative:1.038146\n"
     ]
    }
   ],
   "source": [
    "rmse2 = rmse_filtered(y_hat2, y_true2)\n",
    "print(\"RMSE for PCC user-based collaborative:%f\" %rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and join table to get the movie name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cols = ['item_id', 'movie_title', 'release_date', 'video_release_date', 'URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
    "          'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "df_movieinfo = pd.read_csv('u.item',  sep='|', names=m_cols, encoding='latin-1')\n",
    "\n",
    "# creating a new copy so that any changes to original data 'u.item' will not affect the train and test set below\n",
    "full_movie = df_movieinfo.loc[:, ['item_id', 'movie_title']]\n",
    "full_movie.columns = ['item_id', 'movie_title']\n",
    "full_movie['item_id'] = pd.to_numeric(full_movie['item_id'])\n",
    "\n",
    "movie_mergedf = pd.merge(movieTest, full_movie, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of 10 random users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pool = random.sample(set(movieTest.user_id), 10)\n",
    "print(user_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_mean = movieTrain.groupby('user_id', sort=True).agg(rating_mean=('rating', 'mean'))\n",
    "pearson_mat_all = IUM_train.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = int(input(\"User ID: \"))\n",
    "\n",
    "# indexing\n",
    "idx = movie_mergedf[movie_mergedf['user_id'] == x]\n",
    "\n",
    "# pairs of userid and movieid of the target user in the testing set\n",
    "UI_pairs_target = zip(idx.user_id.values, idx.item_id.values, idx.movie_title.values, idx.rating.values)\n",
    "\n",
    "y_hat3 = []\n",
    "movie_title = []\n",
    "movie_ID = []\n",
    "y_true3 = []\n",
    "\n",
    "for j,k,l,m in UI_pairs_target:\n",
    "    y_hat3.append(round(prediction(j, k, IUM_train, train_user_mean, pearson_mat_all)))\n",
    "    y_true3.append(float(m))\n",
    "    movie_ID.append(k)\n",
    "    movie_title.append(l)\n",
    "    \n",
    "\n",
    "d = {'movie_ID':movie_ID, 'movie_title': movie_title, 'predicted_rating': y_hat3, 'actual_rating': y_true3}\n",
    "\n",
    "# df of matching movie_title\n",
    "tabular = pd.DataFrame(data=d).sort_values(by='predicted_rating', ascending=False).head(3)\n",
    "\n",
    "\n",
    "print(\"\\nTop 3 movies of user with User ID = {0}:\\n\".format(x))\n",
    "print(tabular)\n",
    "\n",
    "\n",
    "rmse3 = rmse_filtered(y_hat3, y_true3)\n",
    "print(\"\\nRMSE for User {0}: {1}\".format(x, rmse3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same training and testing set as above.\n",
    "Like Question 1, we set the minimum number of ratings = 20 for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IUM_train_new = movieTrain.pivot_table(index='item_id', columns='user_id', values='rating')\n",
    "\n",
    "# conditional indexing\n",
    "idx1 = train_mean_filtered['item_id'].values\n",
    "\n",
    "# filtered user-item matrix, with item_id as index\n",
    "IUM_train_new = IUM_train_new.loc[idx1]\n",
    "\n",
    "# UIM_train_new = IUM_train_new.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_IUM = csr_matrix(IUM_train_new.fillna(0))\n",
    "item_sim = cosine_similarity(sparse_IUM)\n",
    "item_sim_df = pd.DataFrame(item_sim, columns=IUM_train_new.index, index=IUM_train_new.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction1(userid, itemid, IUM, w=20):\n",
    "    if itemid in IUM.index:\n",
    "        # items rated by user in the training data set\n",
    "        idxx = IUM.loc[:, userid].dropna().index\n",
    "        cosine_df_0 = item_sim_df.loc[item_sim_df.index.isin(idxx)]\n",
    "        \n",
    "        \n",
    "        cosine_df = pd.DataFrame(cosine_df_0[itemid].sort_values(ascending=False)) \n",
    "        cosine_df.reset_index(level=0, inplace=True)\n",
    "        cosine_df.columns = ['item_id','similarity']\n",
    "\n",
    "        # getting similarities of w nearest items, excluding the target item itself\n",
    "        cosine_df = cosine_df[cosine_df['item_id'] != itemid].head(w)\n",
    "        \n",
    "        cosine_sim = cosine_df['similarity'].values\n",
    "        sum_cosine_sim = cosine_sim.sum()\n",
    "        \n",
    "        # getting actual ratings of w nearest items\n",
    "        cosine_rating = IUM.loc[list(cosine_df['item_id']), userid]\n",
    "        \n",
    "        #predicted ratings\n",
    "        y_hat4 = np.dot(cosine_sim, cosine_rating) / sum_cosine_sim\n",
    "        \n",
    "        return y_hat4\n",
    "    else:\n",
    "        return 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# w nearest items\n",
    "w=20\n",
    "\n",
    "\n",
    "UI_pairs_new = zip(movieTest['user_id'].values, movieTest['item_id'].values)\n",
    "y_hat4 = []\n",
    "\n",
    "for j,k in UI_pairs_new:\n",
    "    y_hat4.append(prediction1(j, k, IUM_train_new, w))\n",
    "\n",
    "y_hat4 = np.array(y_hat4)\n",
    "y_true4 = np.array(movieTest['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse4 = rmse_filtered(y_hat4, y_true4)\n",
    "print(\"RMSE for item-based (cosine similarity) collaborative filtering:%f\" %rmse4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
